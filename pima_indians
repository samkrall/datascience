#import/install packages
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

#load data
df = pd.read_csv('C:/Users/samdk/Desktop/data science/data/diabetes.csv')

'''
Notes
-From head/tail, data is loaded correctly
-shape is 768x9
-info - no missing values, every column is int/float, lots of zeros tho...
-pregnancies could be recodesd
-outcome is a binary, logistic regression or decision tree...
'''
df.dtypes

df.describe()

#correlation
df.corr(method='pearson')

#data skew
df.skew()

#histograms of all columns
#note the "0" values for glucose, BP, skin thickness, insulin, BMI... shouldnt be there
df.hist(figsize=(20, 10))

#density plot of all columns
df.plot(kind='density', figsize = (20, 10), subplots=True, layout=(3,3), sharex=False)

#box/whisker of all columns
df.plot(kind='box', figsize = (20, 10), subplots=True, layout=(3,3), sharex=False, sharey=False) 

#data wrangling
df.info()
df.head()

#track zeros, replace with medians for all continious columns
column_list = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']
for i in column_list:
    df[i].replace(to_replace = 0,value = np.nan, inplace=True)
    df[i].fillna(df[i].median(), inplace=True)
    
#preprocessing data
X = df.iloc[:, 0:8]
y = df.iloc[:, 8]

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
rescaledX = scaler.fit_transform(X) 

#kfold cross validation
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
folds = 5
kfold = KFold(n_splits = folds, random_state = 1, shuffle=True)

#testing out some ml algos
#create lists for relevant model metrics, this will be used to compare different models
model = []
model_mean = []
model_std = []

#logistic regression
from sklearn.linear_model import  LogisticRegression 
method = LogisticRegression(random_state=1) 
results = cross_val_score(method, rescaledX, y, cv=kfold)

#adding to end of list of model metrics
model.append(method)
model_mean.append(round(results.mean(), 4))
model_std.append(round(results.std(), 4))

#decision tree classifier
from sklearn.tree import DecisionTreeClassifier
method = DecisionTreeClassifier(random_state=1) 
results = cross_val_score(method, rescaledX, y, cv=kfold)
model.append(method)
model_mean.append(round(results.mean(), 4))
model_std.append(round(results.std(), 4))

#random forest classifier
from sklearn.ensemble import RandomForestClassifier
method = RandomForestClassifier(random_state=1) 
results = cross_val_score(method, X, y, cv=kfold)
model.append(method)
model_mean.append(round(results.mean(), 4))
model_std.append(round(results.std(), 4))

#gradient boosting classifier
from sklearn.ensemble import GradientBoostingClassifier
method = GradientBoostingClassifier(random_state=1) 
results = cross_val_score(method, X, y, cv=kfold)
model.append(method)
model_mean.append(round(results.mean(), 4))
model_std.append(round(results.std(), 4))

#XG Boost Classifier
import xgboost as xgb
XGB = xgb.XGBClassifier
method = XGB(random_state=1) 
results = cross_val_score(method, X, y, cv=kfold)
model.append(method)
model_mean.append(round(results.mean(), 4))
model_std.append(round(results.std(), 4))

#create df of model results, logreg works best without any tuning, random forest and gradient boosting close behind
models = pd.DataFrame({
    'Model' : model,
    'Score' : model_mean,
    'std' : model_std
})

models.sort_values(by = 'Score', ascending = False)

#gradient boosting, random forest and logistic regression are giving us the best values, lets tune the hyperparameters

#logistic regression hyperparameterization

best_score = 0
from sklearn.linear_model import  LogisticRegression 
for C in [0.1, 1.0, 10.0, 100.0]:
    logreg = LogisticRegression(random_state = 1, C=C)
    scores = cross_val_score(logreg, rescaledX, y, cv=5)
    score = np.mean(scores)
    if score > best_score:
        best_score = score
        best_parameters = {'C':C}
        best_std = np.std(scores)
print(best_score)
print(best_std)
print(best_parameters)

#random forest hyperparameterization
from sklearn.ensemble import RandomForestClassifier
best_score = 0
for max_depth in [3, 5, 7, 9]:
    for n_estimators in [10, 50, 100, 500]:
        rfc = RandomForestClassifier(random_state = 1, max_depth=max_depth, n_estimators = n_estimators)
        scores = cross_val_score(rfc, X, y, cv=5)
        score = np.mean(scores)
        if score > best_score:
            best_score = score
            best_parameters = {'max depth': max_depth, 'n estimators':n_estimators}
            best_std = np.std(scores)
    
print(best_score)
print(best_std)
print(best_parameters)

from sklearn.ensemble import GradientBoostingClassifier
best_score = 0


for learning_rate in [0.1, 0.5, 1, 10, 100]:
    for n_estimators in [1, 25, 50, 100, 250, 500, 1000]:
        for max_leaf_nodes in [2, 3, 4, 5]:
            gbc = GradientBoostingClassifier(random_state=1, learning_rate=learning_rate, n_estimators=n_estimators, max_leaf_nodes=max_leaf_nodes)
            scores = cross_val_score(gbc, X, y, cv = 5)
            score = np.mean(scores)
            if score > best_score:
                best_score = score
                best_std = np.std(scores)
                best_parameters = {'learning rate':learning_rate, 'n estimators':n_estimators, 'max leaf nodes': max_leaf_nodes}
                
                
print(best_score)
print(best_std)
print(best_parameters)

#logisticregression/gradientboosting classifier give similar output, logistic regression is faster, we'll use that one!
